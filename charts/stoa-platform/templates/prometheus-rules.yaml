{{- if .Values.monitoring.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "stoa-platform.fullname" . }}-slo-rules
  namespace: {{ .Values.monitoring.namespace | default "monitoring" }}
  labels:
    {{- include "stoa-platform.labels" . | nindent 4 }}
    prometheus: k8s
    role: alert-rules
spec:
  groups:
    # =========================================================================
    # SLO Recording Rules
    # =========================================================================
    - name: slo-recording
      interval: 30s
      rules:
        # ---------------------------------------------------------------------
        # Availability Metrics
        # ---------------------------------------------------------------------
        # Overall API availability (non-5xx / total)
        - record: slo:api_availability:ratio
          expr: |
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway",status!~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway"}[5m]))

        # Per-service availability
        - record: slo:api_availability:ratio_by_service
          expr: |
            sum by (job) (rate(http_requests_total{status!~"5.."}[5m]))
            /
            sum by (job) (rate(http_requests_total[5m]))

        # Availability over 1 hour (for trending)
        - record: slo:api_availability:ratio_1h
          expr: |
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway",status!~"5.."}[1h]))
            /
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway"}[1h]))

        # ---------------------------------------------------------------------
        # Latency Metrics
        # ---------------------------------------------------------------------
        # p50 latency
        - record: slo:api_latency_p50:seconds
          expr: |
            histogram_quantile(0.50,
              sum by (le) (rate(http_request_duration_seconds_bucket{job=~"control-plane-api|mcp-gateway"}[5m]))
            )

        # p95 latency (SLO target)
        - record: slo:api_latency_p95:seconds
          expr: |
            histogram_quantile(0.95,
              sum by (le) (rate(http_request_duration_seconds_bucket{job=~"control-plane-api|mcp-gateway"}[5m]))
            )

        # p99 latency (SLO target)
        - record: slo:api_latency_p99:seconds
          expr: |
            histogram_quantile(0.99,
              sum by (le) (rate(http_request_duration_seconds_bucket{job=~"control-plane-api|mcp-gateway"}[5m]))
            )

        # Per-service latency
        - record: slo:api_latency_p95:seconds_by_service
          expr: |
            histogram_quantile(0.95,
              sum by (job, le) (rate(http_request_duration_seconds_bucket[5m]))
            )

        # ---------------------------------------------------------------------
        # Error Rate Metrics
        # ---------------------------------------------------------------------
        # Overall error rate (5xx only)
        - record: slo:api_error_rate:ratio
          expr: |
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway",status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway"}[5m]))

        # Per-service error rate
        - record: slo:api_error_rate:ratio_by_service
          expr: |
            sum by (job) (rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum by (job) (rate(http_requests_total[5m]))

        # ---------------------------------------------------------------------
        # Throughput Metrics
        # ---------------------------------------------------------------------
        # Request rate
        - record: slo:api_request_rate:per_second
          expr: |
            sum(rate(http_requests_total{job=~"control-plane-api|mcp-gateway"}[5m]))

        # Per-service request rate
        - record: slo:api_request_rate:per_second_by_service
          expr: |
            sum by (job) (rate(http_requests_total[5m]))

        # ---------------------------------------------------------------------
        # Error Budget Metrics
        # ---------------------------------------------------------------------
        # Error budget remaining (monthly, based on 99.9% SLO)
        # Formula: 1 - ((1 - actual_availability) / (1 - target_slo))
        - record: slo:error_budget:remaining_ratio
          expr: |
            1 - (
              (1 - slo:api_availability:ratio_1h)
              /
              (1 - 0.999)
            )

    # =========================================================================
    # SLO Alerting Rules
    # =========================================================================
    - name: slo-alerts
      rules:
        # ---------------------------------------------------------------------
        # Availability Alerts
        # ---------------------------------------------------------------------
        - alert: SLOAvailabilityBreach
          expr: slo:api_availability:ratio < 0.999
          for: 5m
          labels:
            severity: critical
            channel: ops-alerts
            slo: availability
          annotations:
            summary: "API availability below 99.9% SLO"
            description: "Current availability is {{ "{{" }} $value | humanizePercentage {{ "}}" }}. SLO target is 99.9%."
            runbook_url: "https://github.com/example/stoa/docs/runbooks/critical/availability-breach.md"

        - alert: SLOAvailabilityWarning
          expr: slo:api_availability:ratio < 0.995 and slo:api_availability:ratio >= 0.999
          for: 10m
          labels:
            severity: warning
            channel: ops-alerts
            slo: availability
          annotations:
            summary: "API availability approaching SLO threshold"
            description: "Current availability is {{ "{{" }} $value | humanizePercentage {{ "}}" }}. Getting close to 99.9% SLO."

        # ---------------------------------------------------------------------
        # Latency Alerts
        # ---------------------------------------------------------------------
        - alert: SLOLatencyP95Breach
          expr: slo:api_latency_p95:seconds > 0.5
          for: 5m
          labels:
            severity: warning
            channel: ops-alerts
            slo: latency
          annotations:
            summary: "API p95 latency exceeds 500ms SLO"
            description: "Current p95 latency is {{ "{{" }} $value | humanizeDuration {{ "}}" }}. SLO target is 500ms."

        - alert: SLOLatencyP99Breach
          expr: slo:api_latency_p99:seconds > 1
          for: 5m
          labels:
            severity: warning
            channel: ops-alerts
            slo: latency
          annotations:
            summary: "API p99 latency exceeds 1000ms SLO"
            description: "Current p99 latency is {{ "{{" }} $value | humanizeDuration {{ "}}" }}. SLO target is 1000ms."

        - alert: SLOLatencyCritical
          expr: slo:api_latency_p95:seconds > 2
          for: 2m
          labels:
            severity: critical
            channel: ops-alerts
            slo: latency
          annotations:
            summary: "API latency critically high"
            description: "p95 latency is {{ "{{" }} $value | humanizeDuration {{ "}}" }}. Service may be degraded."

        # ---------------------------------------------------------------------
        # Error Rate Alerts
        # ---------------------------------------------------------------------
        - alert: SLOErrorRateBreach
          expr: slo:api_error_rate:ratio > 0.001
          for: 5m
          labels:
            severity: critical
            channel: ops-alerts
            slo: error_rate
          annotations:
            summary: "API error rate exceeds 0.1% SLO"
            description: "Current error rate is {{ "{{" }} $value | humanizePercentage {{ "}}" }}. SLO target is < 0.1%."

        - alert: SLOErrorRateWarning
          expr: slo:api_error_rate:ratio > 0.0005 and slo:api_error_rate:ratio <= 0.001
          for: 10m
          labels:
            severity: warning
            channel: ops-alerts
            slo: error_rate
          annotations:
            summary: "API error rate approaching SLO threshold"
            description: "Current error rate is {{ "{{" }} $value | humanizePercentage {{ "}}" }}."

        # ---------------------------------------------------------------------
        # Error Budget Alerts
        # ---------------------------------------------------------------------
        - alert: ErrorBudgetLow
          expr: slo:error_budget:remaining_ratio < 0.2 and slo:error_budget:remaining_ratio >= 0.05
          for: 15m
          labels:
            severity: warning
            channel: ops-alerts
            slo: error_budget
          annotations:
            summary: "Error budget below 20%"
            description: "Only {{ "{{" }} $value | humanizePercentage {{ "}}" }} of monthly error budget remaining. Consider freezing non-critical changes."

        - alert: ErrorBudgetExhausted
          expr: slo:error_budget:remaining_ratio < 0.05
          for: 5m
          labels:
            severity: critical
            channel: ops-alerts
            slo: error_budget
          annotations:
            summary: "Error budget nearly exhausted"
            description: "Only {{ "{{" }} $value | humanizePercentage {{ "}}" }} of error budget remaining. Prioritize reliability work."

        # ---------------------------------------------------------------------
        # Component-Specific Alerts
        # ---------------------------------------------------------------------
        - alert: ControlPlaneAPIDown
          expr: up{job="control-plane-api"} == 0
          for: 1m
          labels:
            severity: critical
            channel: ops-alerts
            component: control-plane-api
          annotations:
            summary: "Control-Plane API is down"
            description: "Control-Plane API has been unreachable for more than 1 minute."

        - alert: MCPGatewayDown
          expr: up{job="mcp-gateway"} == 0
          for: 1m
          labels:
            severity: critical
            channel: ops-alerts
            component: mcp-gateway
          annotations:
            summary: "MCP Gateway is down"
            description: "MCP Gateway has been unreachable for more than 1 minute."

        - alert: HighRequestRate
          expr: slo:api_request_rate:per_second > 500
          for: 5m
          labels:
            severity: warning
            channel: ops-alerts
          annotations:
            summary: "High request rate detected"
            description: "Current request rate is {{ "{{" }} $value {{ "}}" }} req/s. Consider scaling."
{{- end }}
